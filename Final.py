# -*- coding: utf-8 -*-
"""Untitled3.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1gvA_TPrE48on05TF_SU5Rr4sx018YT85
"""

# =========================
# 0. Imports & Config
# =========================
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

from collections import Counter

from sklearn.model_selection import train_test_split, StratifiedKFold
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.metrics import (
    accuracy_score, precision_score, recall_score,
    f1_score, roc_auc_score, confusion_matrix,
    classification_report, roc_curve
)

# If imblearn is missing in Colab:  !pip install imbalanced-learn
from imblearn.over_sampling import SMOTE

import tensorflow as tf
from tensorflow.keras.models import Sequential, Model
from tensorflow.keras.layers import Dense, Dropout, BatchNormalization, Activation
from tensorflow.keras.callbacks import EarlyStopping
from tensorflow.keras import regularizers

# Reproducibility
SEED = 42
np.random.seed(SEED)
tf.random.set_seed(SEED)

CSV_PATH = "DiaBD_A Diabetes Dataset for Enhanced Risk Analysis and Research in Bangladesh.csv"

# =========================
# 1. Load & Preprocess Data
# =========================
df = pd.read_csv(CSV_PATH)

# Clean column names
df.columns = df.columns.str.strip().str.lower()

# Detect target column (any col containing 'diabet')
target_candidates = [col for col in df.columns if 'diabet' in col]
if not target_candidates:
    raise ValueError("Could not find target column containing 'diabet' in the column names.")
target_col = target_candidates[0]
print("Detected target column:", target_col)

# Missing values check
missing_values = df.isnull().sum()
if missing_values.sum() == 0:
    print("No missing values found in the dataset.")
else:
    print("⚠️ Missing values detected in the dataset:")
    print(missing_values[missing_values > 0])

# Fill missing values (no inplace to avoid FutureWarning)
for col in df.columns:
    if df[col].dtype == 'object':
        df[col] = df[col].fillna(df[col].mode()[0])
    else:
        df[col] = df[col].fillna(df[col].mean())

# Encode categorical columns
for col in df.select_dtypes(include='object').columns:
    df[col] = LabelEncoder().fit_transform(df[col])

# Separate features and target
X = df.drop(columns=[target_col])
y = df[target_col].astype(int)

# Scale features
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

print("\nFinal feature matrix shape:", X_scaled.shape)
print("Feature columns:", list(X.columns))

# =========================
# 2. Class Distribution (Skewness)
# =========================
print("\nClass distribution (full data):", Counter(y))

plt.figure(figsize=(4, 3))
sns.countplot(x=y)
plt.title("Target Class Distribution (Full Data)")
plt.xlabel("Class (0 = Non-diabetic, 1 = Diabetic)")
plt.ylabel("Count")
plt.tight_layout()
plt.show()

# =========================
# 3. Train / Test Split
# =========================
X_train, X_test, y_train, y_test = train_test_split(
    X_scaled, y,
    test_size=0.2,
    random_state=SEED,
    stratify=y
)

print("\nClass distribution (train before SMOTE):", Counter(y_train))
print("Class distribution (test):", Counter(y_test))

# =========================
# 4. SMOTE Data Augmentation (Handle Imbalance)
# =========================
imbal_ratio = Counter(y_train)
minority_cls = min(imbal_ratio, key=imbal_ratio.get)
majority_cls = max(imbal_ratio, key=imbal_ratio.get)
print(f"\nImbalance check on train set:")
print(f"  Majority class: {majority_cls}")
print(f"  Minority class: {minority_cls}")
print(f"  Counts: {imbal_ratio}")

smote = SMOTE(random_state=SEED)
X_train_res, y_train_res = smote.fit_resample(X_train, y_train)

print("\nAfter SMOTE (train):", Counter(y_train_res))

plt.figure(figsize=(4, 3))
sns.countplot(x=y_train_res)
plt.title("Class Distribution After SMOTE (Train)")
plt.xlabel("Class")
plt.ylabel("Count")
plt.tight_layout()
plt.show()

# =========================
# 5. 4-Hidden-Layer ReLU Model (BN + L1/L2 + Dropout)
# =========================
def build_relu_4layer_bn_l1l2(input_dim, l1=1e-5, l2=1e-4, dropout_rate=0.3):
    """
    4-hidden-layer ReLU MLP:
    - Batch Normalization
    - L1/L2 regularization
    - Dropout
    """
    model = Sequential()

    # Layer 1
    model.add(Dense(
        128, input_dim=input_dim,
        kernel_regularizer=regularizers.l1_l2(l1, l2)
    ))
    model.add(BatchNormalization())
    model.add(Activation('relu'))
    model.add(Dropout(dropout_rate))

    # Layer 2
    model.add(Dense(
        64,
        kernel_regularizer=regularizers.l1_l2(l1, l2)
    ))
    model.add(BatchNormalization())
    model.add(Activation('relu'))
    model.add(Dropout(dropout_rate))

    # Layer 3
    model.add(Dense(
        32,
        kernel_regularizer=regularizers.l1_l2(l1, l2)
    ))
    model.add(BatchNormalization())
    model.add(Activation('relu'))
    model.add(Dropout(dropout_rate))

    # Layer 4
    model.add(Dense(
        16,
        kernel_regularizer=regularizers.l1_l2(l1, l2)
    ))
    model.add(BatchNormalization())
    model.add(Activation('relu'))
    model.add(Dropout(dropout_rate))

    # Output
    model.add(Dense(1, activation='sigmoid'))

    model.compile(
        optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),
        loss='binary_crossentropy',
        metrics=['accuracy']
    )
    return model

# =========================
# 6. 5-Fold Stratified K-Fold CV (On SMOTE Train)
# =========================
EPOCHS = 50
BATCH_SIZE = 32
ES = EarlyStopping(
    monitor='val_loss',
    patience=5,
    restore_best_weights=True,
    verbose=0
)

skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=SEED)

fold_results = []
fold_no = 1

X_k = X_train_res
y_k = y_train_res.values if isinstance(y_train_res, pd.Series) else y_train_res

for train_index, val_index in skf.split(X_k, y_k):
    print(f"\n========== Fold {fold_no} ==========")

    X_tr, X_val = X_k[train_index], X_k[val_index]
    y_tr, y_val = y_k[train_index], y_k[val_index]

    model = build_relu_4layer_bn_l1l2(input_dim=X_k.shape[1])

    history = model.fit(
        X_tr, y_tr,
        validation_data=(X_val, y_val),
        epochs=EPOCHS,
        batch_size=BATCH_SIZE,
        callbacks=[ES],
        verbose=0
    )

    y_val_proba = model.predict(X_val).ravel()
    y_val_pred = (y_val_proba >= 0.5).astype(int)

    acc = accuracy_score(y_val, y_val_pred)
    prec = precision_score(y_val, y_val_pred, zero_division=0)
    rec = recall_score(y_val, y_val_pred, zero_division=0)
    f1 = f1_score(y_val, y_val_pred, zero_division=0)
    auc = roc_auc_score(y_val, y_val_proba)

    print(f"Fold {fold_no} -> "
          f"Accuracy={acc:.4f}, Precision={prec:.4f}, Recall={rec:.4f}, "
          f"F1={f1:.4f}, AUC={auc:.4f}")

    fold_results.append({
        "fold": fold_no,
        "accuracy": acc,
        "precision": prec,
        "recall": rec,
        "f1": f1,
        "auc": auc
    })

    fold_no += 1

fold_df = pd.DataFrame(fold_results)
print("\n===== 5-Fold CV Results (Validation) =====")
print(fold_df)
print("\nMean metrics across folds:")
print(fold_df.mean(numeric_only=True))

# =========================
# 7. Final Model on Full SMOTE Train + Test Evaluation
# =========================
final_model = build_relu_4layer_bn_l1l2(input_dim=X_k.shape[1])

history_final = final_model.fit(
    X_k, y_k,
    epochs=EPOCHS,
    batch_size=BATCH_SIZE,
    validation_split=0.1,
    callbacks=[ES],
    verbose=0
)

# Predictions on test set
y_test_proba = final_model.predict(X_test).ravel()
y_test_pred = (y_test_proba >= 0.5).astype(int)

test_acc = accuracy_score(y_test, y_test_pred)
test_prec = precision_score(y_test, y_test_pred, zero_division=0)
test_rec = recall_score(y_test, y_test_pred, zero_division=0)
test_f1 = f1_score(y_test, y_test_pred, zero_division=0)
test_auc = roc_auc_score(y_test, y_test_proba)
cm = confusion_matrix(y_test, y_test_pred)

print("\n===== Final Test Set Performance =====")
print(f"Accuracy : {test_acc:.4f}")
print(f"Precision: {test_prec:.4f}")
print(f"Recall   : {test_rec:.4f}")
print(f"F1-score : {test_f1:.4f}")
print(f"ROC-AUC  : {test_auc:.4f}")
print("\nConfusion Matrix:\n", cm)
print("\nClassification Report:\n", classification_report(y_test, y_test_pred, zero_division=0))

# =========================
# 8. Plots: Confusion Matrix, ROC, Accuracy & Loss Curves
# =========================

# Confusion Matrix (counts + %)
cm_counts = cm
cm_percent = (cm_counts / cm_counts.sum()) * 100

plt.figure(figsize=(6, 4))
ax = sns.heatmap(
    cm_counts,
    annot=True,
    fmt='d',
    cmap='Blues',
    xticklabels=['Pred: 0', 'Pred: 1'],
    yticklabels=['Actual: 0', 'Actual: 1']
)
for i in range(cm_counts.shape[0]):
    for j in range(cm_counts.shape[1]):
        ax.text(j + 0.5, i + 0.8,
                f"{cm_percent[i, j]:.1f}%",
                color='red', ha="center", fontsize=9)
plt.title("Confusion Matrix (Counts + % of Total)")
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.tight_layout()
plt.show()

# ROC Curve (Test Set)
fpr, tpr, _ = roc_curve(y_test, y_test_proba)
plt.figure(figsize=(6, 4))
plt.plot(fpr, tpr, label=f"AUC = {test_auc:.4f}")
plt.plot([0, 1], [0, 1], 'k--', alpha=0.6)
plt.xlabel("False Positive Rate")
plt.ylabel("True Positive Rate")
plt.title("ROC Curve (Test Set)")
plt.legend(loc="lower right")
plt.grid(True)
plt.tight_layout()
plt.show()

# Training vs Validation Accuracy Curve
plt.figure(figsize=(6, 4))
plt.plot(history_final.history['accuracy'], label='Train Accuracy')
plt.plot(history_final.history['val_accuracy'], label='Val Accuracy')
plt.title("Training vs Validation Accuracy")
plt.xlabel("Epoch")
plt.ylabel("Accuracy")
plt.legend()
plt.grid(True)
plt.tight_layout()
plt.show()

# Training vs Validation Loss Curve
plt.figure(figsize=(6, 4))
plt.plot(history_final.history['loss'], label='Train Loss')
plt.plot(history_final.history['val_loss'], label='Val Loss')
plt.title("Training vs Validation Loss")
plt.xlabel("Epoch")
plt.ylabel("Loss")
plt.legend()
plt.grid(True)
plt.tight_layout()
plt.show()

# =========================
# 9. Feature Extraction from Last Hidden Layer
# =========================
# Use last hidden layer (index -2) as feature extractor
feature_extractor = Model(
    inputs=final_model.layers[0].input,     # use first layer's input to avoid "never been called" error
    outputs=final_model.layers[-2].output   # last hidden layer before sigmoid
)

X_features = feature_extractor.predict(X_scaled)
print("\nExtracted feature shape from last hidden layer:", X_features.shape)

# Optional: save features for other ML models
# np.save("diabetes_hidden_features.npy", X_features)
# np.save("diabetes_labels.npy", y.values)